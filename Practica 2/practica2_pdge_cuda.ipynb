{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Práctica 2: GPU Programming (CUDA)"
      ],
      "metadata": {
        "id": "JQu_Ue46PUBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación del entorno"
      ],
      "metadata": {
        "id": "9rl6fk0zP0uX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminación de datos innecesarios creador por Google Collab"
      ],
      "metadata": {
        "id": "LdzBfLv1PjwG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVsaiS2yPB8S"
      },
      "outputs": [],
      "source": [
        "!rm -rf sample_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descarga de `Numba` en caso de no encontrarse en el sistema"
      ],
      "metadata": {
        "id": "SQj2UBZCP-co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jojn0H5qP7Yl",
        "outputId": "59da1414-830b-41f9-db4b-f8ad424c7812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paquetes necesarios para el correcto funcionamiento del código"
      ],
      "metadata": {
        "id": "7eWE1oOQQffG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "import time\n",
        "import math"
      ],
      "metadata": {
        "id": "sU7PX7nzQfA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Compulsory assignment #1: Matrix transpose"
      ],
      "metadata": {
        "id": "Mv5Y84hHQsJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ax = 5\n",
        "Ay = 7\n",
        "Bx = Ay\n",
        "By = Ax\n",
        "\n",
        "def create_matrix_1(Ax: int = 5, Ay: int = 7) -> tuple:\n",
        "  return np.random.rand(Ax, Ay), np.zeros((Ay, Ax))"
      ],
      "metadata": {
        "id": "it6U5yE7QwiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def transpose_parallel(A, B):\n",
        "    i, j = cuda.grid(2)\n",
        "\n",
        "    if i < B.shape[0] and j < B.shape[1]:\n",
        "        B[i, j] = A[j, i]\n",
        "\n",
        "def transpose_sequential(A, B):\n",
        "    for i in range(0, B.shape[0]):\n",
        "        for j in range(0, B.shape[1]):\n",
        "            B[i, j] = A[j, i]"
      ],
      "metadata": {
        "id": "i8oCTH7GRpOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A, B_seq = create_matrix_1()\n",
        "\n",
        "t_start = time.time()\n",
        "transpose_sequential(A, B_seq)\n",
        "t_finish = time.time()\n",
        "\n",
        "t_cpu = t_finish - t_start\n",
        "\n",
        "print(f\"A = {A}\")\n",
        "print(f\"B sequential = {B_seq}\")\n",
        "print(f\"Tiempo ejecución cpu = {t_cpu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDg9rArtSCfl",
        "outputId": "28e80676-792c-4a79-99ec-e1b9f6dc3d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A = [[0.59519143 0.35179901 0.88770519 0.85486685 0.38753975 0.98115259\n",
            "  0.49717322]\n",
            " [0.34034105 0.31260797 0.0868682  0.26890184 0.73219868 0.1489382\n",
            "  0.5872679 ]\n",
            " [0.77548061 0.55378198 0.5088211  0.17974199 0.22764901 0.68396361\n",
            "  0.82230342]\n",
            " [0.8567986  0.6150403  0.76329363 0.58079591 0.3485574  0.39368057\n",
            "  0.51019182]\n",
            " [0.60263113 0.27987086 0.68787657 0.56656794 0.18172897 0.61801314\n",
            "  0.71105428]]\n",
            "B sequential = [[0.59519143 0.34034105 0.77548061 0.8567986  0.60263113]\n",
            " [0.35179901 0.31260797 0.55378198 0.6150403  0.27987086]\n",
            " [0.88770519 0.0868682  0.5088211  0.76329363 0.68787657]\n",
            " [0.85486685 0.26890184 0.17974199 0.58079591 0.56656794]\n",
            " [0.38753975 0.73219868 0.22764901 0.3485574  0.18172897]\n",
            " [0.98115259 0.1489382  0.68396361 0.39368057 0.61801314]\n",
            " [0.49717322 0.5872679  0.82230342 0.51019182 0.71105428]]\n",
            "Tiempo ejecución cpu = 0.00013113021850585938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Explicaciones pal iñigo\n",
        "\n",
        "_, B_par = create_matrix_1()\n",
        "\n",
        "# Esto sirve para meter el la vram de la gpu las matrices\n",
        "A_device = cuda.to_device(A)\n",
        "B_device = cuda.to_device(B_par)\n",
        "\n",
        "# Para el uso optimo de la gpu, tenemos que tener en cuenta que\n",
        "# para maxima eficiencia de un warp (conjunto de hilos) este tiene\n",
        "# que tener 32, como estamos tratando datos en 2D, dividimos esos\n",
        "# 32 hilos en dos\n",
        "threads_per_block = (16, 16)\n",
        "# Para determinar el número de bloques que debemos usar simplemente\n",
        "# dividimos el eje entre el número de hilos del eje aplicando la función\n",
        "# techo para que no se queden posiciones libres sin hilos asignados\n",
        "# (aunque para ello debamos crear hilos que no se vayan a usar)\n",
        "blocks_X = math.ceil(B_par.shape[0] / threads_per_block[0])\n",
        "blocks_Y = math.ceil(B_par.shape[1] / threads_per_block[1])\n",
        "blocks_total = (blocks_X, blocks_Y)\n",
        "\n",
        "t_start = time.time()\n",
        "# Llamamos a la función de la transpuesta en paralelo, para ello, hay\n",
        "# que indicar el número de bloques que se van a usar en cada dimensión,\n",
        "# junto al número de hilos que va a tener cada bloque\n",
        "transpose_parallel[blocks_total, threads_per_block](A_device, B_device)\n",
        "\n",
        "# Esto es MUY importante poque como lo que acabamos de hacer es \"una\n",
        "# pelea salvaje en la jungla\" por los calculos de los hilos, debemos\n",
        "# asegurarnos que todos acaban, para eso sirve esta función, en caso de\n",
        "# que un hilo falte por realizar sus calculos no se continuará.\n",
        "cuda.synchronize()\n",
        "t_finish = time.time()\n",
        "\n",
        "# Tras realizar todos los calculos necesarios, se copia de la vram de la gpu\n",
        "# a la ram de la cpu\n",
        "B_par = B_device.copy_to_host()\n",
        "\n",
        "t_gpu = t_finish - t_start\n",
        "\n",
        "print(f\"A = {A}\")\n",
        "print(f\"B parallel = {B_par}\")\n",
        "print(f\"Tiempo ejecución gpu = {t_gpu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbjE5DA4Uwj4",
        "outputId": "47e871ca-0d59-4ab3-ca81-68cba8b990f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A = [[0.59519143 0.35179901 0.88770519 0.85486685 0.38753975 0.98115259\n",
            "  0.49717322]\n",
            " [0.34034105 0.31260797 0.0868682  0.26890184 0.73219868 0.1489382\n",
            "  0.5872679 ]\n",
            " [0.77548061 0.55378198 0.5088211  0.17974199 0.22764901 0.68396361\n",
            "  0.82230342]\n",
            " [0.8567986  0.6150403  0.76329363 0.58079591 0.3485574  0.39368057\n",
            "  0.51019182]\n",
            " [0.60263113 0.27987086 0.68787657 0.56656794 0.18172897 0.61801314\n",
            "  0.71105428]]\n",
            "B parallel = [[0.59519143 0.34034105 0.77548061 0.8567986  0.60263113]\n",
            " [0.35179901 0.31260797 0.55378198 0.6150403  0.27987086]\n",
            " [0.88770519 0.0868682  0.5088211  0.76329363 0.68787657]\n",
            " [0.85486685 0.26890184 0.17974199 0.58079591 0.56656794]\n",
            " [0.38753975 0.73219868 0.22764901 0.3485574  0.18172897]\n",
            " [0.98115259 0.1489382  0.68396361 0.39368057 0.61801314]\n",
            " [0.49717322 0.5872679  0.82230342 0.51019182 0.71105428]]\n",
            "Tiempo ejecución gpu = 0.0005588531494140625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Pal iñigo: Como puedes darte cuenta, el tiempo de ejecución de la gpu es más alto que el de la cpu cuando por lógica debería ser al revés al hacerlo en paralelo, esto se debe a que al ser un tamaño de matrix pequeño, es mayor el peso de creación de hilos y ponerlos en ejecución que el de la propia ejecución de la traspuesta. Si se decidiese poner un tamaño mucho más grande de matriz se podría ver como esto se cumple.`"
      ],
      "metadata": {
        "id": "Zzwz3R9tbBCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "speedup = t_cpu / t_gpu\n",
        "\n",
        "print(f\"Speedup = {speedup}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBnV5NOwbubU",
        "outputId": "ad6620cd-1a02-4d2a-bb1f-2bf52c1930a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speedup = 0.23464163822525597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Compulsory assignment #2: Average Rows/Cols I"
      ],
      "metadata": {
        "id": "MU-MpuZbcsYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_matrix_2(Ax: int, Ay: int):\n",
        "    A = np.random.rand(Ax, Ay)\n",
        "    B = np.zeros(Ay)\n",
        "    return A, B\n",
        "\n",
        "def Avg_Rows(input, output):\n",
        "    for y in range(input.shape[1]):\n",
        "        output[y] = 0.0\n",
        "        for x in range(input.shape[0]):\n",
        "            output[y] += input[x, y]\n",
        "        output[y] /= input.shape[0]\n",
        "\n",
        "@cuda.jit\n",
        "def Avg_Rows_parallel(A, B, Ax):\n",
        "    y = cuda.grid(1)\n",
        "    if y < B.shape[0]:\n",
        "        temp_sum = 0.0\n",
        "        for x in range(Ax):\n",
        "            temp_sum += A[x, y]\n",
        "        B[y] = temp_sum / Ax"
      ],
      "metadata": {
        "id": "b-a13FuccopR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pal iñigo: haz tu esto según los comentarios que te he puesto en el ejercicio anterior"
      ],
      "metadata": {
        "id": "MsUpbLgbdA5J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}