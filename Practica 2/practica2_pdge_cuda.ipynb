{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Práctica 2: GPU Programming (CUDA)"
      ],
      "metadata": {
        "id": "JQu_Ue46PUBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación del entorno"
      ],
      "metadata": {
        "id": "9rl6fk0zP0uX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminación de datos innecesarios creados por Google Collab"
      ],
      "metadata": {
        "id": "LdzBfLv1PjwG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qVsaiS2yPB8S"
      },
      "outputs": [],
      "source": [
        "!rm -rf sample_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descarga de `Numba` en caso de no encontrarse en el sistema"
      ],
      "metadata": {
        "id": "SQj2UBZCP-co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jojn0H5qP7Yl",
        "outputId": "31acc5b8-2b01-4b31-d359-58e02147d535"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paquetes necesarios para el correcto funcionamiento del código"
      ],
      "metadata": {
        "id": "7eWE1oOQQffG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda, float32\n",
        "import time\n",
        "import math"
      ],
      "metadata": {
        "id": "sU7PX7nzQfA-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Compulsory assignment #1: Matrix transpose"
      ],
      "metadata": {
        "id": "Mv5Y84hHQsJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ax = 5\n",
        "Ay = 7\n",
        "Bx = Ay\n",
        "By = Ax\n",
        "\n",
        "def create_matrix_1(Ax: int = 5000, Ay: int = 7000) -> tuple:\n",
        "  return np.random.rand(Ax, Ay), np.zeros((Ay, Ax))"
      ],
      "metadata": {
        "id": "it6U5yE7QwiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################# Explicaciones pal iñigo #############################\n",
        "\n",
        "# Esto es un decorador, sirve para que la máquina sepa como\n",
        "# ejecutar el código siguiente, en este caso, sirve para indicarle\n",
        "# que la función se ejecuta en gpu (tarjeta gráfica) (paralelizando)\n",
        "# en vez de en cpu (procesador) (que normalmente es en secuencial)\n",
        "@cuda.jit\n",
        "def transpose_parallel(A, B):\n",
        "    # Esta función sirve para obtener la posición del hilo actual\n",
        "    # en una matriz de 2 dimensiones\n",
        "    i, j = cuda.grid(2)\n",
        "\n",
        "    # Como verás en una explicación después, algunas veces por conveniencia\n",
        "    # se crean hilos de más, haciendo que haya más hilos que posiciones en\n",
        "    # esto puede causar problemas en el acceso a las posiciones, asi que tenemos\n",
        "    # que asegurarnos que el hilo que estamos usando esta dentro de la posición\n",
        "    # de la matriz\n",
        "    if i < B.shape[0] and j < B.shape[1]:\n",
        "        B[i, j] = A[j, i]\n",
        "\n",
        "def transpose_sequential(A, B):\n",
        "    for i in range(0, B.shape[0]):\n",
        "        for j in range(0, B.shape[1]):\n",
        "            B[i, j] = A[j, i]"
      ],
      "metadata": {
        "id": "i8oCTH7GRpOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A, B_seq = create_matrix_1()\n",
        "\n",
        "t_start = time.time()\n",
        "transpose_sequential(A, B_seq)\n",
        "t_finish = time.time()\n",
        "\n",
        "t_cpu = t_finish - t_start\n",
        "\n",
        "print(f\"A = {A}\")\n",
        "print(f\"B sequential = {B_seq}\")\n",
        "print(f\"Tiempo ejecución cpu = {t_cpu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDg9rArtSCfl",
        "outputId": "d859921c-3a77-484b-8b6a-3f68c6c8b45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A = [[0.5332116  0.30702299 0.10948742 ... 0.38825487 0.48623714 0.39570486]\n",
            " [0.87560167 0.82235289 0.05881936 ... 0.28517249 0.229938   0.54345414]\n",
            " [0.17976587 0.6223765  0.46298395 ... 0.76090316 0.98335124 0.16403582]\n",
            " ...\n",
            " [0.6870055  0.42397955 0.1101008  ... 0.70033198 0.324623   0.03178206]\n",
            " [0.74661687 0.26387922 0.1424545  ... 0.03242135 0.19320822 0.19844304]\n",
            " [0.72395246 0.39598295 0.36207431 ... 0.00946169 0.04915248 0.33458658]]\n",
            "B sequential = [[0.5332116  0.87560167 0.17976587 ... 0.6870055  0.74661687 0.72395246]\n",
            " [0.30702299 0.82235289 0.6223765  ... 0.42397955 0.26387922 0.39598295]\n",
            " [0.10948742 0.05881936 0.46298395 ... 0.1101008  0.1424545  0.36207431]\n",
            " ...\n",
            " [0.38825487 0.28517249 0.76090316 ... 0.70033198 0.03242135 0.00946169]\n",
            " [0.48623714 0.229938   0.98335124 ... 0.324623   0.19320822 0.04915248]\n",
            " [0.39570486 0.54345414 0.16403582 ... 0.03178206 0.19844304 0.33458658]]\n",
            "Tiempo ejecución cpu = 14.932383298873901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################# Explicaciones pal iñigo #############################\n",
        "\n",
        "_, B_par = create_matrix_1()\n",
        "\n",
        "# Esto sirve para meter el la vram de la gpu las matrices\n",
        "A_device = cuda.to_device(A)\n",
        "B_device = cuda.to_device(B_par)\n",
        "\n",
        "# Para el uso optimo de la gpu, tenemos que tener en cuenta que\n",
        "# para maxima eficiencia de un warp (conjunto de hilos) este tiene\n",
        "# que tener 32, como estamos tratando datos en 2D, dividimos esos\n",
        "# 32 hilos en dos\n",
        "threads_per_block = (16, 16)\n",
        "# Para determinar el número de bloques que debemos usar simplemente\n",
        "# dividimos el eje entre el número de hilos del eje aplicando la función\n",
        "# techo para que no se queden posiciones libres sin hilos asignados\n",
        "# (aunque para ello debamos crear hilos que no se vayan a usar)\n",
        "blocks_X = math.ceil(B_par.shape[0] / threads_per_block[0])\n",
        "blocks_Y = math.ceil(B_par.shape[1] / threads_per_block[1])\n",
        "blocks_total = (blocks_X, blocks_Y)\n",
        "\n",
        "t_start = time.time()\n",
        "# Llamamos a la función de la transpuesta en paralelo, para ello, hay\n",
        "# que indicar el número de bloques que se van a usar en cada dimensión,\n",
        "# junto al número de hilos que va a tener cada bloque\n",
        "transpose_parallel[blocks_total, threads_per_block](A_device, B_device)\n",
        "\n",
        "# Esto es MUY importante poque como lo que acabamos de hacer es \"una\n",
        "# pelea salvaje en la jungla\" por los calculos de los hilos, debemos\n",
        "# asegurarnos que todos acaban, para eso sirve esta función, en caso de\n",
        "# que un hilo falte por realizar sus calculos no se continuará. Esto en\n",
        "# informática se llama \"Condición de carrera\" y es muy importante para\n",
        "# que funcione bien un codigo\n",
        "cuda.synchronize()\n",
        "t_finish = time.time()\n",
        "\n",
        "# Tras realizar todos los calculos necesarios, se copia de la vram de la gpu\n",
        "# a la ram de la cpu\n",
        "B_par = B_device.copy_to_host()\n",
        "\n",
        "t_gpu = t_finish - t_start\n",
        "\n",
        "print(f\"A = {A}\")\n",
        "print(f\"B parallel = {B_par}\")\n",
        "print(f\"Tiempo ejecución gpu = {t_gpu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbjE5DA4Uwj4",
        "outputId": "1e113ae2-1dbe-444e-aa03-f9a62c88d662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A = [[0.5332116  0.30702299 0.10948742 ... 0.38825487 0.48623714 0.39570486]\n",
            " [0.87560167 0.82235289 0.05881936 ... 0.28517249 0.229938   0.54345414]\n",
            " [0.17976587 0.6223765  0.46298395 ... 0.76090316 0.98335124 0.16403582]\n",
            " ...\n",
            " [0.6870055  0.42397955 0.1101008  ... 0.70033198 0.324623   0.03178206]\n",
            " [0.74661687 0.26387922 0.1424545  ... 0.03242135 0.19320822 0.19844304]\n",
            " [0.72395246 0.39598295 0.36207431 ... 0.00946169 0.04915248 0.33458658]]\n",
            "B parallel = [[0.5332116  0.87560167 0.17976587 ... 0.6870055  0.74661687 0.72395246]\n",
            " [0.30702299 0.82235289 0.6223765  ... 0.42397955 0.26387922 0.39598295]\n",
            " [0.10948742 0.05881936 0.46298395 ... 0.1101008  0.1424545  0.36207431]\n",
            " ...\n",
            " [0.38825487 0.28517249 0.76090316 ... 0.70033198 0.03242135 0.00946169]\n",
            " [0.48623714 0.229938   0.98335124 ... 0.324623   0.19320822 0.04915248]\n",
            " [0.39570486 0.54345414 0.16403582 ... 0.03178206 0.19844304 0.33458658]]\n",
            "Tiempo ejecución gpu = 0.3652820587158203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Pal iñigo: Como puedes darte cuenta, el tiempo de ejecución de la gpu es más alto que el de la cpu cuando por lógica debería ser al revés al hacerlo en paralelo, esto se debe a que al ser un tamaño de matrix pequeño, es mayor el peso de creación de hilos y ponerlos en ejecución que el de la propia ejecución de la traspuesta. Si se decidiese poner un tamaño mucho más grande de matriz se podría ver como esto se cumple.`"
      ],
      "metadata": {
        "id": "Zzwz3R9tbBCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "speedup = t_cpu / t_gpu\n",
        "\n",
        "print(f\"Speedup = {speedup}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBnV5NOwbubU",
        "outputId": "e03c4128-dfc0-42bb-e32e-31883bf49fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speedup = 40.879049333465616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Compulsory assignment #2: Average Rows/Cols I"
      ],
      "metadata": {
        "id": "MU-MpuZbcsYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_matrix_2(Ax: int, Ay: int):\n",
        "    A = np.random.rand(Ax, Ay)\n",
        "    B = np.zeros(Ay)\n",
        "    return A, B\n",
        "\n",
        "def Avg_Rows(input, output):\n",
        "    for y in range(input.shape[1]):\n",
        "        output[y] = 0.0\n",
        "        for x in range(input.shape[0]):\n",
        "            output[y] += input[x, y]\n",
        "        output[y] /= input.shape[0]\n",
        "\n",
        "@cuda.jit\n",
        "def Avg_Rows_parallel(A, B, Ax):\n",
        "    y = cuda.grid(1)\n",
        "    if y < B.shape[0]:\n",
        "        temp_sum = 0.0\n",
        "        for x in range(Ax):\n",
        "            temp_sum += A[x, y]\n",
        "        B[y] = temp_sum / Ax"
      ],
      "metadata": {
        "id": "b-a13FuccopR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pal iñigo: haz tu esto según los comentarios que te he puesto en el ejercicio anterior"
      ],
      "metadata": {
        "id": "MsUpbLgbdA5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Compulsory assignment #3: Average Rows/Cols II (2 points)"
      ],
      "metadata": {
        "id": "m-JgxtFQ6n-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pal iñigo: este tambien lo haces tu que es la segunda parte del anterior\n",
        "# sin embargo, ten en cuenta que se usa MEMORIA COMPARTIDA, puedes mirar el siguiente\n",
        "# ejercicio para saber como usarlo"
      ],
      "metadata": {
        "id": "lablhRLX6rXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Optional assignment #1: Mean_3x3 (2 points)"
      ],
      "metadata": {
        "id": "p8EQzO8h6EuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Mean_3x3_sequential(input, output):\n",
        "    for x in range(input.shape[0]):\n",
        "        for y in range(input.shape[1]):\n",
        "            output[x, y] = 0.0\n",
        "            for i in range(-1, 2):\n",
        "                for j in range(-1, 2):\n",
        "                    if x + i >= 0 and x + i < input.shape[0] and \\\n",
        "                        y + j >= 0 and y + j < input.shape[1]:\n",
        "                        output[x, y] += input[x + i, y + j]\n",
        "            output[x, y] /= 9.0\n",
        "\n",
        "################################ Explicaciones pal iñigo ################################\n",
        "@cuda.jit\n",
        "def Mean_3x3_parallel(input, output, aux):\n",
        "\n",
        "    # Recuerda que cada bloque se encarga de una matriz más pequeña que la matriz inicial\n",
        "    # que nosotros hemos definido de 16x16, en este ejercicio en concreto nos piden\n",
        "    # calcular la media de los números adyacentes a una posición y el valor de la posición\n",
        "    # también (y la media siempre se hace dividiendo entre 9), para hacer el ejercicio más\n",
        "    # sencillo para nosotros y los hilos, he optado por hacer más grande la matriz de memoria\n",
        "    # compartida, haciendo que la matriz sea 2 veces más ancha y larga.\n",
        "    # Ej:\n",
        "    #                                           [[0.         0.         0.         0.         0.        ]\n",
        "    # [[0.98469016 0.22750222 0.99213766]       [0.         0.98469019 0.22750223 0.99213767 0.        ]\n",
        "    # [0.25682925 0.49983334 0.25695094]    ->  [0.         0.25682923 0.49983335 0.25695094 0.        ]\n",
        "    # [0.8441824  0.22751885 0.92430338]]       [0.         0.84418243 0.22751886 0.92430335 0.        ]\n",
        "    #                                           [0.         0.         0.         0.         0.        ]]\n",
        "    shared_mem = cuda.shared.array((16 + 2, 16 + 2), dtype=float32)\n",
        "\n",
        "    # Obtenemos el número total de filas y columnas de la matriz de entrada\n",
        "    rows, cols = input.shape\n",
        "\n",
        "    # Índices GLOBALES del hilo, o sea, índices de la matriz completa, que hay que tener en cuenta\n",
        "    # que no se salga de la propia matriz.\n",
        "    x, y = cuda.grid(2)\n",
        "\n",
        "    # Índices LOCALES del hilo, o sea, índices que usa el hilo para el bloque al cual pertenece\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.threadIdx.y\n",
        "\n",
        "    # Cargamos los datos en memoria compartida, manejando los bordes, haciendo así la transformación\n",
        "    # de arriba\n",
        "    if 0 <= x < rows and 0 <= y < cols:\n",
        "        shared_mem[tx + 1, ty + 1] = input[x, y]\n",
        "    else:\n",
        "        shared_mem[tx + 1, ty + 1] = 0.0\n",
        "\n",
        "    # Cargar las celdas de los bordes\n",
        "    if tx == 0 and x > 0:\n",
        "        shared_mem[0, ty + 1] = input[x - 1, y] if (y < cols) else 0.0\n",
        "    if tx == cuda.blockDim.x - 1 and x < rows - 1:\n",
        "        shared_mem[tx + 2, ty + 1] = input[x + 1, y] if (y < cols) else 0.0\n",
        "    if ty == 0 and y > 0:\n",
        "        shared_mem[tx + 1, 0] = input[x, y - 1] if (x < rows) else 0.0\n",
        "    if ty == cuda.blockDim.y - 1 and y < cols - 1:\n",
        "        shared_mem[tx + 1, ty + 2] = input[x, y + 1] if (x < rows) else 0.0\n",
        "\n",
        "    # Cargar las celdas de las esquinas\n",
        "    if tx == 0 and ty == 0 and x > 0 and y > 0:\n",
        "        shared_mem[0, 0] = input[x - 1, y - 1]\n",
        "    if tx == cuda.blockDim.x - 1 and ty == 0 and x < rows - 1 and y > 0:\n",
        "        shared_mem[tx + 2, 0] = input[x + 1, y - 1]\n",
        "    if tx == 0 and ty == cuda.blockDim.y - 1 and x > 0 and y < cols - 1:\n",
        "        shared_mem[0, ty + 2] = input[x - 1, y + 1]\n",
        "    if tx == cuda.blockDim.x - 1 and ty == cuda.blockDim.y - 1 and x < rows - 1 and y < cols - 1:\n",
        "        shared_mem[tx + 2, ty + 2] = input[x + 1, y + 1]\n",
        "\n",
        "    # Esperamos a que todos los hilos terminen de cargar los datos en la memoria compartida\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    # Calcular la media 3x3 si estamos dentro del rango\n",
        "    if x < rows and y < cols:\n",
        "        if y != 0 and y != cols-1 and x != 0 and x != rows-1:\n",
        "            # Sumar los 8 vecinos y la celda central\n",
        "            total_sum = (shared_mem[tx, ty] + shared_mem[tx, ty + 1] + shared_mem[tx, ty + 2] +\n",
        "                        shared_mem[tx + 1, ty] + shared_mem[tx + 1, ty + 1] + shared_mem[tx + 1, ty + 2] +\n",
        "                        shared_mem[tx + 2, ty] + shared_mem[tx + 2, ty + 1] + shared_mem[tx + 2, ty + 2])\n",
        "        elif y == 0:\n",
        "            total_sum = (shared_mem[tx, ty + 1] + shared_mem[tx, ty + 2] +\n",
        "                        shared_mem[tx + 1, ty + 1] + shared_mem[tx + 1, ty + 2] +\n",
        "                        shared_mem[tx + 2, ty + 1] + shared_mem[tx + 2, ty + 2])\n",
        "        elif y == cols-1:\n",
        "            total_sum = (shared_mem[tx, ty] + shared_mem[tx, ty + 1] +\n",
        "                        shared_mem[tx + 1, ty] + shared_mem[tx + 1, ty + 1] +\n",
        "                        shared_mem[tx + 2, ty] + shared_mem[tx + 2, ty + 1])\n",
        "        elif x == 0:\n",
        "            total_sum = (shared_mem[tx + 1, ty] + shared_mem[tx + 1, ty + 1] + shared_mem[tx + 1, ty + 2] +\n",
        "                        shared_mem[tx + 2, ty] + shared_mem[tx + 2, ty + 1] + shared_mem[tx + 2, ty + 2])\n",
        "        elif x == rows-1:\n",
        "            total_sum = (shared_mem[tx, ty] + shared_mem[tx, ty + 1] + shared_mem[tx, ty + 2] +\n",
        "                        shared_mem[tx + 1, ty] + shared_mem[tx + 1, ty + 1] + shared_mem[tx + 1, ty + 2])\n",
        "\n",
        "        output[x, y] = total_sum / 9.0\n",
        "        aux[x+1,y+1] = shared_mem[tx+1, ty+1]\n",
        "\n"
      ],
      "metadata": {
        "id": "dDP9905M6Kdr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ax = 50\n",
        "Ay = 50\n",
        "A = np.random.rand(Ax, Ay)\n",
        "B = np.zeros_like(A)\n",
        "\n",
        "Mean_3x3_sequential(A, B)\n",
        "print(\"input\\n\", A)\n",
        "print(\"output\\n\", B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOOoL1fg77Fx",
        "outputId": "cfabab29-4b89-4f1f-f9ea-eb22075d86d3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input\n",
            " [[0.75018795 0.27061406 0.04210938 ... 0.9774601  0.38901495 0.0739896 ]\n",
            " [0.44632532 0.10740952 0.79027803 ... 0.60865875 0.47404548 0.35310359]\n",
            " [0.61929449 0.39873305 0.06086447 ... 0.2993003  0.06597863 0.66437202]\n",
            " ...\n",
            " [0.21153505 0.74717074 0.34278181 ... 0.65921429 0.06490726 0.85674045]\n",
            " [0.52539601 0.07231435 0.687105   ... 0.62929564 0.4541948  0.1011438 ]\n",
            " [0.58974546 0.00119948 0.76067248 ... 0.2046277  0.42302088 0.73715959]]\n",
            "output\n",
            " [[0.17494854 0.26743603 0.24140607 ... 0.37910986 0.31958583 0.1433504 ]\n",
            " [0.28806271 0.38731292 0.36541706 ... 0.4836637  0.43399149 0.22450047]\n",
            " [0.26022232 0.41842538 0.34896939 ... 0.4552149  0.53206582 0.35888163]\n",
            " ...\n",
            " [0.21820048 0.43764681 0.43492106 ... 0.46207447 0.44357502 0.19980112]\n",
            " [0.23859568 0.43754671 0.36274663 ... 0.46046782 0.45892271 0.29301853]\n",
            " [0.13207281 0.29293698 0.21195241 ... 0.28543436 0.28327138 0.19061323]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B = np.zeros_like(A)\n",
        "B_aux = np.zeros_like(np.random.rand(Ax+2, Ay+2))\n",
        "\n",
        "A_device = cuda.to_device(A)\n",
        "B_device = cuda.to_device(B)\n",
        "aux_device = cuda.to_device(B_aux)\n",
        "\n",
        "threads_per_block = (16,16)\n",
        "\n",
        "blocks_X = math.ceil(B_aux.shape[0] / threads_per_block[0])\n",
        "blocks_Y = math.ceil(B_aux.shape[1] / threads_per_block[1])\n",
        "blocks_total = (blocks_X, blocks_Y)\n",
        "\n",
        "\n",
        "Mean_3x3_parallel[blocks_total, threads_per_block](A_device, B_device, aux_device)\n",
        "\n",
        "B_par = B_device.copy_to_host()\n",
        "aux = aux_device.copy_to_host()\n",
        "\n",
        "print(\"input\\n\", A)\n",
        "print(\"output\\n\", B_par)\n",
        "print(\"memory\\n\", aux)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoXI_YDgBW6v",
        "outputId": "c6b48933-190a-4f67-94b7-81d6145355db"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 16 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input\n",
            " [[0.75018795 0.27061406 0.04210938 ... 0.9774601  0.38901495 0.0739896 ]\n",
            " [0.44632532 0.10740952 0.79027803 ... 0.60865875 0.47404548 0.35310359]\n",
            " [0.61929449 0.39873305 0.06086447 ... 0.2993003  0.06597863 0.66437202]\n",
            " ...\n",
            " [0.21153505 0.74717074 0.34278181 ... 0.65921429 0.06490726 0.85674045]\n",
            " [0.52539601 0.07231435 0.687105   ... 0.62929564 0.4541948  0.1011438 ]\n",
            " [0.58974546 0.00119948 0.76067248 ... 0.2046277  0.42302088 0.73715959]]\n",
            "output\n",
            " [[0.17494853 0.26743603 0.24140607 ... 0.37910986 0.31958585 0.1433504 ]\n",
            " [0.2880627  0.38731292 0.36541706 ... 0.48366366 0.43399149 0.22450047]\n",
            " [0.2602223  0.41842537 0.34896938 ... 0.45521492 0.53206582 0.35888163]\n",
            " ...\n",
            " [0.21820048 0.43764679 0.43492105 ... 0.46207449 0.44357501 0.19980113]\n",
            " [0.23859567 0.43754673 0.36274664 ... 0.46046782 0.4589227  0.29301855]\n",
            " [0.13207282 0.29293696 0.21195241 ... 0.28543435 0.28327139 0.19061323]]\n",
            "memory\n",
            " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.75018793 0.27061406 ... 0.38901496 0.0739896  0.        ]\n",
            " [0.         0.44632533 0.10740952 ... 0.47404549 0.35310358 0.        ]\n",
            " ...\n",
            " [0.         0.52539599 0.07231435 ... 0.45419481 0.1011438  0.        ]\n",
            " [0.         0.58974546 0.00119948 ... 0.42302087 0.73715961 0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional assignment #2: Reduction approaches (3 points)"
      ],
      "metadata": {
        "id": "jNJsCS_POCqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Reduce_sequential(input, output):\n",
        "    acum = 0\n",
        "    for i in range(input.shape[0]):\n",
        "        acum += input[i]\n",
        "    output[0] = acum\n",
        "\n",
        "\n",
        "N = 40\n",
        "np.random.seed(0)\n",
        "A = np.random.rand(N)\n",
        "output = np.zeros(1)\n",
        "\n",
        "Reduce_sequential(A, output)\n",
        "\n",
        "print(\"Input\\n\", A)\n",
        "print(\"Output\\n\", output[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yK9GZG6Wr7V",
        "outputId": "37fb8135-3a29-4fde-d16d-bcdbca6227ab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input\n",
            " [0.5488135  0.71518937 0.60276338 0.54488318 0.4236548  0.64589411\n",
            " 0.43758721 0.891773   0.96366276 0.38344152 0.79172504 0.52889492\n",
            " 0.56804456 0.92559664 0.07103606 0.0871293  0.0202184  0.83261985\n",
            " 0.77815675 0.87001215 0.97861834 0.79915856 0.46147936 0.78052918\n",
            " 0.11827443 0.63992102 0.14335329 0.94466892 0.52184832 0.41466194\n",
            " 0.26455561 0.77423369 0.45615033 0.56843395 0.0187898  0.6176355\n",
            " 0.61209572 0.616934   0.94374808 0.6818203 ]\n",
            "Output\n",
            " 22.988006826188062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduction #1: Interleaved addressing with divergent branching"
      ],
      "metadata": {
        "id": "QUrHrvK1Wk-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################ Explicaciones pal iñigo ################################\n",
        "\n",
        "@cuda.jit\n",
        "def reduce_interleaved_divergent(input, output):\n",
        "    # Se piden crear dos listas en memoria compartida\n",
        "    sSrc = cuda.shared.array(16, dtype=float32)\n",
        "    sDst = cuda.shared.array(16, dtype=float32)\n",
        "\n",
        "    # Id del hilo en el bloque\n",
        "    tid = cuda.threadIdx.x\n",
        "    # Id del bloque del programa\n",
        "    bid = cuda.blockIdx.x\n",
        "    # Nº de hilos que tiene el bloque\n",
        "    bx = cuda.blockDim.x\n",
        "\n",
        "    # Copiamos los datos del input a la memoria compartida\n",
        "    if tid + bid * bx < input.size:\n",
        "        # Este es el metodo por excelencia para guardar los datos en una lista\n",
        "        # en el que no se solapa la posición con ningun otro hilo, en caso de\n",
        "        # no comprenderlo dimelo\n",
        "        sSrc[tid] = input[tid + bid * bx]\n",
        "    else:\n",
        "        # En caso de que el hilo no tenga posición del array no pone valor\n",
        "        sSrc[tid] = 0\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    # El stride es la división del hilo que se va multiplicando por 2 para que\n",
        "    # se vaya minimizando el uso de hilos, haciendo en cada iteración:\n",
        "    # nºhilos/2 -> nºhilos/4 ...\n",
        "    stride = 2\n",
        "    while stride <= bx:\n",
        "        if tid % stride == 0:\n",
        "            if tid + stride // 2 < bx:\n",
        "                # sSrc[tid + stride // 2] lo dan en el enunciado\n",
        "                sDst[tid] = sSrc[tid] + sSrc[tid + stride // 2]\n",
        "        cuda.syncthreads()\n",
        "\n",
        "        # Se copia la lista resultado a la lista de elementos para la siguiente\n",
        "        # iteración\n",
        "        sSrc[tid] = sDst[tid]\n",
        "        cuda.syncthreads()\n",
        "\n",
        "        # Como se indica arriba, se multiplica stride * 2\n",
        "        stride *= 2\n",
        "\n",
        "    # El primer hilo de todos tiene el privilegio de guardar el resultado\n",
        "    if tid == 0:\n",
        "        cuda.atomic.add(output, 0, sDst[0])"
      ],
      "metadata": {
        "id": "twNQAnUJOCHb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = np.zeros(1)\n",
        "\n",
        "input_device = cuda.to_device(A)\n",
        "output_device = cuda.to_device(output)\n",
        "\n",
        "threads_per_block = 16\n",
        "blocks_per_grid = math.ceil(N + threads_per_block - 1 / threads_per_block)\n",
        "\n",
        "reduce_interleaved_divergent[blocks_per_grid, threads_per_block](input_device, output_device)\n",
        "\n",
        "output = output_device.copy_to_host()\n",
        "\n",
        "print(\"Input\\n\", A)\n",
        "print(\"Output\\n\", output[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXk9iR_PQMnZ",
        "outputId": "78896b8b-0ead-4511-92c1-2e8c98677fa3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 56 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input\n",
            " [0.5488135  0.71518937 0.60276338 0.54488318 0.4236548  0.64589411\n",
            " 0.43758721 0.891773   0.96366276 0.38344152 0.79172504 0.52889492\n",
            " 0.56804456 0.92559664 0.07103606 0.0871293  0.0202184  0.83261985\n",
            " 0.77815675 0.87001215 0.97861834 0.79915856 0.46147936 0.78052918\n",
            " 0.11827443 0.63992102 0.14335329 0.94466892 0.52184832 0.41466194\n",
            " 0.26455561 0.77423369 0.45615033 0.56843395 0.0187898  0.6176355\n",
            " 0.61209572 0.616934   0.94374808 0.6818203 ]\n",
            "Output\n",
            " 2.1071193516254425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduction #2: Interleaved addressing with no divergent branching"
      ],
      "metadata": {
        "id": "Mfa_IwVUXVxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def reduce_interleaved_no_divergent(input, output):\n",
        "\n",
        "    sSrc = cuda.shared.array(16, dtype=float32)\n",
        "\n",
        "    tid = cuda.threadIdx.x\n",
        "    bid = cuda.blockIdx.x\n",
        "    bx = cuda.blockDim.x\n",
        "\n",
        "    if tid + bid * bx < input.size:\n",
        "        sSrc[tid] = input[tid + bid * bx]\n",
        "    else:\n",
        "        sSrc[tid] = 0\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    stride = 1\n",
        "    while stride < bx:\n",
        "        index = 2 * stride * tid\n",
        "        if index + stride < bx:\n",
        "            sSrc[index] += sSrc[index + stride]\n",
        "        cuda.syncthreads()\n",
        "\n",
        "        stride *= 2\n",
        "\n",
        "    if tid == 0:\n",
        "        cuda.atomic.add(output, 0, sSrc[0])"
      ],
      "metadata": {
        "id": "L36WqhwRXHj0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = np.zeros(1)\n",
        "\n",
        "input_device = cuda.to_device(A)\n",
        "output_device = cuda.to_device(output)\n",
        "\n",
        "threads_per_block = 16\n",
        "blocks_per_grid = math.ceil(N + threads_per_block - 1 / threads_per_block)\n",
        "\n",
        "reduce_interleaved_no_divergent[blocks_per_grid, threads_per_block](input_device, output_device)\n",
        "\n",
        "output = output_device.copy_to_host()\n",
        "\n",
        "print(\"Input\\n\", A)\n",
        "print(\"Output\\n\", output[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS3a2SYlXLAY",
        "outputId": "70db3d29-3807-49a7-dea3-539ea2b9c985"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input\n",
            " [0.5488135  0.71518937 0.60276338 0.54488318 0.4236548  0.64589411\n",
            " 0.43758721 0.891773   0.96366276 0.38344152 0.79172504 0.52889492\n",
            " 0.56804456 0.92559664 0.07103606 0.0871293  0.0202184  0.83261985\n",
            " 0.77815675 0.87001215 0.97861834 0.79915856 0.46147936 0.78052918\n",
            " 0.11827443 0.63992102 0.14335329 0.94466892 0.52184832 0.41466194\n",
            " 0.26455561 0.77423369 0.45615033 0.56843395 0.0187898  0.6176355\n",
            " 0.61209572 0.616934   0.94374808 0.6818203 ]\n",
            "Output\n",
            " 22.988006591796875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 56 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduction #3: Sequential addressing"
      ],
      "metadata": {
        "id": "U1v0BTGbX3kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def reduce_sequential_addressing(input, output):\n",
        "    # Falta por hacer\n",
        "    pass"
      ],
      "metadata": {
        "id": "_fvyOQncX1I3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = np.zeros(1)\n",
        "\n",
        "input_device = cuda.to_device(A)\n",
        "output_device = cuda.to_device(output)\n",
        "\n",
        "threads_per_block = 16\n",
        "blocks_per_grid = math.ceil(N + threads_per_block - 1 / threads_per_block)\n",
        "\n",
        "reduce_sequential_addressing[blocks_per_grid, threads_per_block](input_device, output_device)\n",
        "\n",
        "output = output_device.copy_to_host()\n",
        "\n",
        "print(\"Input\\n\", A)\n",
        "print(\"Output\\n\", output[0])"
      ],
      "metadata": {
        "id": "riO6z0hCX76H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduction #4: Atomic addition"
      ],
      "metadata": {
        "id": "M-wRqWP7YXAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def reduce_atomic_addition(input, output):\n",
        "\n",
        "    partial_sum = cuda.shared.array(16, dtype=float32)\n",
        "\n",
        "    tid = cuda.threadIdx.x\n",
        "    bid = cuda.blockIdx.x\n",
        "    bx = cuda.blockDim.x\n",
        "\n",
        "    if tid == 0:\n",
        "        partial_sum[0] = 0.0\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    if tid + bid * bx < input.size:\n",
        "        cuda.atomic.add(partial_sum, 0, input[tid + bid * bx])\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    if tid == 0:\n",
        "        cuda.atomic.add(output, 0, partial_sum[0])"
      ],
      "metadata": {
        "id": "I3BOSmW_YQvQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = np.zeros(1)\n",
        "\n",
        "input_device = cuda.to_device(A)\n",
        "output_device = cuda.to_device(output)\n",
        "\n",
        "threads_per_block = 16\n",
        "blocks_per_grid = math.ceil(N + threads_per_block - 1 / threads_per_block)\n",
        "\n",
        "reduce_atomic_addition[blocks_per_grid, threads_per_block](input_device, output_device)\n",
        "\n",
        "output = output_device.copy_to_host()\n",
        "\n",
        "print(\"Input\\n\", A)\n",
        "print(\"Output\\n\", output[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSUni8DwYea8",
        "outputId": "1a2c6d39-3bbb-4c0c-959e-3d0f71d6f868"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input\n",
            " [0.5488135  0.71518937 0.60276338 0.54488318 0.4236548  0.64589411\n",
            " 0.43758721 0.891773   0.96366276 0.38344152 0.79172504 0.52889492\n",
            " 0.56804456 0.92559664 0.07103606 0.0871293  0.0202184  0.83261985\n",
            " 0.77815675 0.87001215 0.97861834 0.79915856 0.46147936 0.78052918\n",
            " 0.11827443 0.63992102 0.14335329 0.94466892 0.52184832 0.41466194\n",
            " 0.26455561 0.77423369 0.45615033 0.56843395 0.0187898  0.6176355\n",
            " 0.61209572 0.616934   0.94374808 0.6818203 ]\n",
            "Output\n",
            " 22.988006114959717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 56 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    }
  ]
}