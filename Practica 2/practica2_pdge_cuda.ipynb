{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQu_Ue46PUBG"
      },
      "source": [
        "# Práctica 2: GPU Programming (CUDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rl6fk0zP0uX"
      },
      "source": [
        "## Preparación del entorno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdzBfLv1PjwG"
      },
      "source": [
        "Eliminación de datos innecesarios creados por Google Collab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qVsaiS2yPB8S"
      },
      "outputs": [],
      "source": [
        "!rm -rf sample_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQj2UBZCP-co"
      },
      "source": [
        "Descarga de `Numba` en caso de no encontrarse en el sistema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jojn0H5qP7Yl",
        "outputId": "31acc5b8-2b01-4b31-d359-58e02147d535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install numba --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eWE1oOQQffG"
      },
      "source": [
        "Paquetes necesarios para el correcto funcionamiento del código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sU7PX7nzQfA-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import cuda, float32\n",
        "import time\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv5Y84hHQsJC"
      },
      "source": [
        "## 2.1 Compulsory assignment #1: Matrix transpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it6U5yE7QwiG"
      },
      "outputs": [],
      "source": [
        "Ax = 5\n",
        "Ay = 7\n",
        "Bx = Ay\n",
        "By = Ax\n",
        "\n",
        "def create_matrix_1(Ax: int = 5000, Ay: int = 7000) -> tuple:\n",
        "  return np.random.rand(Ax, Ay), np.zeros((Ay, Ax))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8oCTH7GRpOm"
      },
      "outputs": [],
      "source": [
        "############################# Explicaciones pal iñigo #############################\n",
        "\n",
        "# Esto es un decorador, sirve para que la máquina sepa como\n",
        "# ejecutar el código siguiente, en este caso, sirve para indicarle\n",
        "# que la función se ejecuta en gpu (tarjeta gráfica) (paralelizando)\n",
        "# en vez de en cpu (procesador) (que normalmente es en secuencial)\n",
        "@cuda.jit\n",
        "def transpose_parallel(A, B):\n",
        "    # Esta función sirve para obtener la posición del hilo actual\n",
        "    # en una matriz de 2 dimensiones\n",
        "    i, j = cuda.grid(2)\n",
        "\n",
        "    # Como verás en una explicación después, algunas veces por conveniencia\n",
        "    # se crean hilos de más, haciendo que haya más hilos que posiciones en\n",
        "    # esto puede causar problemas en el acceso a las posiciones, asi que tenemos\n",
        "    # que asegurarnos que el hilo que estamos usando esta dentro de la posición\n",
        "    # de la matriz\n",
        "    if i < B.shape[0] and j < B.shape[1]:\n",
        "        B[i, j] = A[j, i]\n",
        "\n",
        "def transpose_sequential(A, B):\n",
        "    for i in range(0, B.shape[0]):\n",
        "        for j in range(0, B.shape[1]):\n",
        "            B[i, j] = A[j, i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDg9rArtSCfl",
        "outputId": "d859921c-3a77-484b-8b6a-3f68c6c8b45b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A = [[0.5332116  0.30702299 0.10948742 ... 0.38825487 0.48623714 0.39570486]\n",
            " [0.87560167 0.82235289 0.05881936 ... 0.28517249 0.229938   0.54345414]\n",
            " [0.17976587 0.6223765  0.46298395 ... 0.76090316 0.98335124 0.16403582]\n",
            " ...\n",
            " [0.6870055  0.42397955 0.1101008  ... 0.70033198 0.324623   0.03178206]\n",
            " [0.74661687 0.26387922 0.1424545  ... 0.03242135 0.19320822 0.19844304]\n",
            " [0.72395246 0.39598295 0.36207431 ... 0.00946169 0.04915248 0.33458658]]\n",
            "B sequential = [[0.5332116  0.87560167 0.17976587 ... 0.6870055  0.74661687 0.72395246]\n",
            " [0.30702299 0.82235289 0.6223765  ... 0.42397955 0.26387922 0.39598295]\n",
            " [0.10948742 0.05881936 0.46298395 ... 0.1101008  0.1424545  0.36207431]\n",
            " ...\n",
            " [0.38825487 0.28517249 0.76090316 ... 0.70033198 0.03242135 0.00946169]\n",
            " [0.48623714 0.229938   0.98335124 ... 0.324623   0.19320822 0.04915248]\n",
            " [0.39570486 0.54345414 0.16403582 ... 0.03178206 0.19844304 0.33458658]]\n",
            "Tiempo ejecución cpu = 14.932383298873901\n"
          ]
        }
      ],
      "source": [
        "A, B_seq = create_matrix_1()\n",
        "\n",
        "t_start = time.time()\n",
        "transpose_sequential(A, B_seq)\n",
        "t_finish = time.time()\n",
        "\n",
        "t_cpu = t_finish - t_start\n",
        "\n",
        "print(f\"A = {A}\")\n",
        "print(f\"B sequential = {B_seq}\")\n",
        "print(f\"Tiempo ejecución cpu = {t_cpu}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbjE5DA4Uwj4",
        "outputId": "1e113ae2-1dbe-444e-aa03-f9a62c88d662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A = [[0.5332116  0.30702299 0.10948742 ... 0.38825487 0.48623714 0.39570486]\n",
            " [0.87560167 0.82235289 0.05881936 ... 0.28517249 0.229938   0.54345414]\n",
            " [0.17976587 0.6223765  0.46298395 ... 0.76090316 0.98335124 0.16403582]\n",
            " ...\n",
            " [0.6870055  0.42397955 0.1101008  ... 0.70033198 0.324623   0.03178206]\n",
            " [0.74661687 0.26387922 0.1424545  ... 0.03242135 0.19320822 0.19844304]\n",
            " [0.72395246 0.39598295 0.36207431 ... 0.00946169 0.04915248 0.33458658]]\n",
            "B parallel = [[0.5332116  0.87560167 0.17976587 ... 0.6870055  0.74661687 0.72395246]\n",
            " [0.30702299 0.82235289 0.6223765  ... 0.42397955 0.26387922 0.39598295]\n",
            " [0.10948742 0.05881936 0.46298395 ... 0.1101008  0.1424545  0.36207431]\n",
            " ...\n",
            " [0.38825487 0.28517249 0.76090316 ... 0.70033198 0.03242135 0.00946169]\n",
            " [0.48623714 0.229938   0.98335124 ... 0.324623   0.19320822 0.04915248]\n",
            " [0.39570486 0.54345414 0.16403582 ... 0.03178206 0.19844304 0.33458658]]\n",
            "Tiempo ejecución gpu = 0.3652820587158203\n"
          ]
        }
      ],
      "source": [
        "############################# Explicaciones pal iñigo #############################\n",
        "\n",
        "_, B_par = create_matrix_1()\n",
        "\n",
        "# Esto sirve para meter el la vram de la gpu las matrices\n",
        "A_device = cuda.to_device(A)\n",
        "B_device = cuda.to_device(B_par)\n",
        "\n",
        "# Para el uso optimo de la gpu, tenemos que tener en cuenta que\n",
        "# para maxima eficiencia de un warp (conjunto de hilos) este tiene\n",
        "# que tener 32, como estamos tratando datos en 2D, dividimos esos\n",
        "# 32 hilos en dos\n",
        "threads_per_block = (16, 16)\n",
        "# Para determinar el número de bloques que debemos usar simplemente\n",
        "# dividimos el eje entre el número de hilos del eje aplicando la función\n",
        "# techo para que no se queden posiciones libres sin hilos asignados\n",
        "# (aunque para ello debamos crear hilos que no se vayan a usar)\n",
        "blocks_X = math.ceil(B_par.shape[0] / threads_per_block[0])\n",
        "blocks_Y = math.ceil(B_par.shape[1] / threads_per_block[1])\n",
        "blocks_total = (blocks_X, blocks_Y)\n",
        "\n",
        "t_start = time.time()\n",
        "# Llamamos a la función de la transpuesta en paralelo, para ello, hay\n",
        "# que indicar el número de bloques que se van a usar en cada dimensión,\n",
        "# junto al número de hilos que va a tener cada bloque\n",
        "transpose_parallel[blocks_total, threads_per_block](A_device, B_device)\n",
        "\n",
        "# Esto es MUY importante poque como lo que acabamos de hacer es \"una\n",
        "# pelea salvaje en la jungla\" por los calculos de los hilos, debemos\n",
        "# asegurarnos que todos acaban, para eso sirve esta función, en caso de\n",
        "# que un hilo falte por realizar sus calculos no se continuará. Esto en\n",
        "# informática se llama \"Condición de carrera\" y es muy importante para\n",
        "# que funcione bien un codigo\n",
        "cuda.synchronize()\n",
        "t_finish = time.time()\n",
        "\n",
        "# Tras realizar todos los calculos necesarios, se copia de la vram de la gpu\n",
        "# a la ram de la cpu\n",
        "B_par = B_device.copy_to_host()\n",
        "\n",
        "t_gpu = t_finish - t_start\n",
        "\n",
        "print(f\"A = {A}\")\n",
        "print(f\"B parallel = {B_par}\")\n",
        "print(f\"Tiempo ejecución gpu = {t_gpu}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzwz3R9tbBCC"
      },
      "source": [
        "`Pal iñigo: Como puedes darte cuenta, el tiempo de ejecución de la gpu es más alto que el de la cpu cuando por lógica debería ser al revés al hacerlo en paralelo, esto se debe a que al ser un tamaño de matrix pequeño, es mayor el peso de creación de hilos y ponerlos en ejecución que el de la propia ejecución de la traspuesta. Si se decidiese poner un tamaño mucho más grande de matriz se podría ver como esto se cumple.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBnV5NOwbubU",
        "outputId": "e03c4128-dfc0-42bb-e32e-31883bf49fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Speedup = 40.879049333465616\n"
          ]
        }
      ],
      "source": [
        "speedup = t_cpu / t_gpu\n",
        "\n",
        "print(f\"Speedup = {speedup}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU-MpuZbcsYf"
      },
      "source": [
        "## 2.2 Compulsory assignment #2: Average Rows/Cols I"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b-a13FuccopR"
      },
      "outputs": [],
      "source": [
        "def avg_Cols_sequential(input, output):\n",
        "    for y in range(input.shape[1]):\n",
        "        output[y] = 0.0\n",
        "        for x in range(input.shape[0]):\n",
        "            output[y] += input[x, y]\n",
        "        output[y] /= input.shape[0]\n",
        "\n",
        "def avg_Rows_sequential(input, output):\n",
        "    #TODO (esto es un comentario especial de informáticos\n",
        "    # sirve para indicar que queda por hacer cosas (del verbo en ingles\n",
        "    # to do), siempre no sepas que hacer busca un TODO por el código)\n",
        "    pass\n",
        "\n",
        "############################# Explicaciones pal iñigo #############################\n",
        "\n",
        "@cuda.jit\n",
        "def avg_Cols_parallel(input, output):\n",
        "    # En este ejercicio no necesita que hagamos un recorrido en matriz,\n",
        "    # sino en filas, por lo tanto, solo usaremos un eje\n",
        "    y = cuda.grid(1)\n",
        "    \n",
        "    if y < input.shape[1]:\n",
        "        sum_val = 0.0\n",
        "        # Cada hilo suma todos los valores de la columna que le ha tocado\n",
        "        for x in range(input.shape[0]):\n",
        "            sum_val += input[x, y]\n",
        "        # Y luego divide por el número de elementos en la columna\n",
        "        output[y] = sum_val / input.shape[0]\n",
        "\n",
        "@cuda.jit\n",
        "def avg_Rows_parallel(input, output):\n",
        "    #TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MsUpbLgbdA5J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input\n",
            " [[0.5488135  0.71518937 0.60276338 0.54488318 0.4236548  0.64589411\n",
            "  0.43758721]\n",
            " [0.891773   0.96366276 0.38344152 0.79172504 0.52889492 0.56804456\n",
            "  0.92559664]\n",
            " [0.07103606 0.0871293  0.0202184  0.83261985 0.77815675 0.87001215\n",
            "  0.97861834]\n",
            " [0.79915856 0.46147936 0.78052918 0.11827443 0.63992102 0.14335329\n",
            "  0.94466892]\n",
            " [0.52184832 0.41466194 0.26455561 0.77423369 0.45615033 0.56843395\n",
            "  0.0187898 ]]\n",
            "output\n",
            " [0.56652589 0.52842455 0.41030162 0.61234724 0.56535556 0.55914761\n",
            " 0.66105218]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "Ax = 5\n",
        "Ay = 7\n",
        "\n",
        "A = np.random.rand(Ax, Ay)\n",
        "B = np.zeros(Ay)\n",
        "\n",
        "#TODO: calculo de tiempo en cpu\n",
        "\n",
        "avg_Cols_sequential(A, B)\n",
        "\n",
        "print(\"input\\n\", A)\n",
        "print(\"output\\n\", B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pal iñigo: haz tu la ejecución del codigo en paralelo según los comentarios que te he puesto en el ejercicio anterior\n",
        "# IMPORTANTE: el codigo que da el profesor esta AL REVÉS, en la funcion de media de columnas hace de filas y viceversa\n",
        "\n",
        "#A no hace falta ponerlo porque ya esta inicializado de antes\n",
        "B = np.zeros(Ay)\n",
        "\n",
        "#TODO: ejecución del calculo en paralelo\n",
        "\n",
        "#TODO: calculo del tiempo en gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO: calculo del speedup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-JgxtFQ6n-g"
      },
      "source": [
        "## 2.3 Compulsory assignment #3: Average Rows/Cols II (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lablhRLX6rXR"
      },
      "outputs": [],
      "source": [
        "# Pal iñigo: este tambien lo haces tu que es la segunda parte del anterior\n",
        "# sin embargo, ten en cuenta que se usa MEMORIA COMPARTIDA, puedes mirar el siguiente\n",
        "# ejercicio para saber como usarlo\n",
        "\n",
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8EQzO8h6EuU"
      },
      "source": [
        "## 2.4 Optional assignment #1: Mean_3x3 (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "dDP9905M6Kdr"
      },
      "outputs": [],
      "source": [
        "def Mean_3x3_sequential(input, output):\n",
        "    for x in range(input.shape[0]):\n",
        "        for y in range(input.shape[1]):\n",
        "            output[x, y] = 0.0\n",
        "            for i in range(-1, 2):\n",
        "                for j in range(-1, 2):\n",
        "                    if x + i >= 0 and x + i < input.shape[0] and \\\n",
        "                        y + j >= 0 and y + j < input.shape[1]:\n",
        "                        output[x, y] += input[x + i, y + j]\n",
        "            output[x, y] /= 9.0\n",
        "\n",
        "################################ Explicaciones pal iñigo ################################\n",
        "@cuda.jit\n",
        "def Mean_3x3_parallel(input, output, aux):\n",
        "\n",
        "    # Recuerda que cada bloque se encarga de una matriz más pequeña que la matriz inicial\n",
        "    # que nosotros hemos definido de 16x16, en este ejercicio en concreto nos piden\n",
        "    # calcular la media de los números adyacentes a una posición y el valor de la posición\n",
        "    # también (y la media siempre se hace dividiendo entre 9), para hacer el ejercicio más\n",
        "    # sencillo para nosotros y los hilos, he optado por hacer más grande la matriz de memoria\n",
        "    # compartida, haciendo que la matriz sea 2 veces más ancha y larga.\n",
        "    # Ej:\n",
        "    #                                           [[0.         0.         0.         0.         0.        ]\n",
        "    # [[0.98469016 0.22750222 0.99213766]       [0.         0.98469019 0.22750223 0.99213767 0.        ]\n",
        "    # [0.25682925 0.49983334 0.25695094]    ->  [0.         0.25682923 0.49983335 0.25695094 0.        ]\n",
        "    # [0.8441824  0.22751885 0.92430338]]       [0.         0.84418243 0.22751886 0.92430335 0.        ]\n",
        "    #                                           [0.         0.         0.         0.         0.        ]]\n",
        "    shared_mem = cuda.shared.array((16 + 2, 16 + 2), dtype=float32)\n",
        "\n",
        "    # Obtenemos el número total de filas y columnas de la matriz de entrada\n",
        "    rows, cols = input.shape\n",
        "\n",
        "    # Índices GLOBALES del hilo, o sea, índices de la matriz completa, que hay que tener en cuenta\n",
        "    # que no se salga de la propia matriz.\n",
        "    x, y = cuda.grid(2)\n",
        "\n",
        "    # Índices LOCALES del hilo, o sea, índices que usa el hilo para el bloque al cual pertenece\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.threadIdx.y\n",
        "\n",
        "    # Cargamos los datos en memoria compartida, manejando los bordes, haciendo así la transformación\n",
        "    # de arriba\n",
        "    if 0 <= x < rows and 0 <= y < cols:\n",
        "        shared_mem[tx + 1, ty + 1] = input[x, y]\n",
        "    else:\n",
        "        shared_mem[tx + 1, ty + 1] = 0.0\n",
        "\n",
        "    # Cargar las celdas de los bordes\n",
        "    if tx == 0 and x > 0:\n",
        "        shared_mem[0, ty + 1] = input[x - 1, y] if (y < cols) else 0.0\n",
        "    if tx == cuda.blockDim.x - 1 and x < rows - 1:\n",
        "        shared_mem[tx + 2, ty + 1] = input[x + 1, y] if (y < cols) else 0.0\n",
        "    if ty == 0 and y > 0:\n",
        "        shared_mem[tx + 1, 0] = input[x, y - 1] if (x < rows) else 0.0\n",
        "    if ty == cuda.blockDim.y - 1 and y < cols - 1:\n",
        "        shared_mem[tx + 1, ty + 2] = input[x, y + 1] if (x < rows) else 0.0\n",
        "\n",
        "    # Cargar las celdas de las esquinas\n",
        "    if tx == 0 and ty == 0 and x > 0 and y > 0:\n",
        "        shared_mem[0, 0] = input[x - 1, y - 1]\n",
        "    if tx == cuda.blockDim.x - 1 and ty == 0 and x < rows - 1 and y > 0:\n",
        "        shared_mem[tx + 2, 0] = input[x + 1, y - 1]\n",
        "    if tx == 0 and ty == cuda.blockDim.y - 1 and x > 0 and y < cols - 1:\n",
        "        shared_mem[0, ty + 2] = input[x - 1, y + 1]\n",
        "    if tx == cuda.blockDim.x - 1 and ty == cuda.blockDim.y - 1 and x < rows - 1 and y < cols - 1:\n",
        "        shared_mem[tx + 2, ty + 2] = input[x + 1, y + 1]\n",
        "\n",
        "    # Esperamos a que todos los hilos terminen de cargar los datos en la memoria compartida\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    # Calcular la media 3x3 si estamos dentro del rango\n",
        "    if x < rows and y < cols:\n",
        "        total_sum = 0.0\n",
        "        if y != 0 and y != cols-1 and x != 0 and x != rows-1:\n",
        "            # Sumar los 8 vecinos y la celda central\n",
        "            total_sum = (shared_mem[tx, ty] + shared_mem[tx, ty + 1] + shared_mem[tx, ty + 2] +\n",
        "                        shared_mem[tx + 1, ty] + shared_mem[tx + 1, ty + 1] + shared_mem[tx + 1, ty + 2] +\n",
        "                        shared_mem[tx + 2, ty] + shared_mem[tx + 2, ty + 1] + shared_mem[tx + 2, ty + 2])\n",
        "        elif y == 0 and x == rows-1:\n",
        "            total_sum = (shared_mem[tx, ty + 1]     +  shared_mem[tx, ty + 2] +\n",
        "                         shared_mem[tx + 1, ty + 1] +  shared_mem[tx + 1, ty + 2])\n",
        "        elif y == cols-1 and x == rows-1:\n",
        "            total_sum = (shared_mem[tx, ty]    +  shared_mem[tx, ty + 1] +\n",
        "                        shared_mem[tx + 1, ty] +  shared_mem[tx + 1, ty + 1])\n",
        "        elif y == 0:\n",
        "            total_sum = (shared_mem[tx, ty + 1] + shared_mem[tx, ty + 2] +\n",
        "                        shared_mem[tx + 1, ty + 1] + shared_mem[tx + 1, ty + 2] +\n",
        "                        shared_mem[tx + 2, ty + 1] + shared_mem[tx + 2, ty + 2])\n",
        "        elif y == cols-1:\n",
        "            total_sum = (shared_mem[tx, ty] + shared_mem[tx, ty + 1] +\n",
        "                        shared_mem[tx + 1, ty] + shared_mem[tx + 1, ty + 1] +\n",
        "                        shared_mem[tx + 2, ty] + shared_mem[tx + 2, ty + 1])\n",
        "        elif x == 0:\n",
        "            total_sum = (shared_mem[tx + 1, ty] + shared_mem[tx + 1, ty + 1] + shared_mem[tx + 1, ty + 2] +\n",
        "                        shared_mem[tx + 2, ty] + shared_mem[tx + 2, ty + 1] + shared_mem[tx + 2, ty + 2])\n",
        "        elif x == rows-1:\n",
        "            total_sum = (shared_mem[tx, ty] + shared_mem[tx, ty + 1] + shared_mem[tx, ty + 2] +\n",
        "                        shared_mem[tx + 1, ty] + shared_mem[tx + 1, ty + 1] + shared_mem[tx + 1, ty + 2])\n",
        "\n",
        "        output[x, y] = total_sum / 9.0\n",
        "        aux[x+1,y+1] = shared_mem[tx+1, ty+1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOOoL1fg77Fx",
        "outputId": "e69cb266-c2fa-4c48-fe18-f49026663818"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input\n",
            " [[0.25343217 0.88453917 0.11655505 ... 0.58155069 0.3772749  0.65740254]\n",
            " [0.54746975 0.53939572 0.66098    ... 0.38421246 0.60397874 0.98743678]\n",
            " [0.49014738 0.66282884 0.66250477 ... 0.3242048  0.09154355 0.27138173]\n",
            " ...\n",
            " [0.04962472 0.31829174 0.66001656 ... 0.53019427 0.87677016 0.35581574]\n",
            " [0.82315862 0.5766718  0.82800638 ... 0.83812446 0.10684913 0.36040019]\n",
            " [0.43248398 0.68363741 0.8608957  ... 0.8023918  0.60685719 0.13238978]]\n",
            "output\n",
            " [[0.24720409 0.33359687 0.34379535 ... 0.35326124 0.39909512 0.29178811]\n",
            " [0.37531256 0.53531698 0.51184538 ... 0.4056532  0.47544291 0.33211314]\n",
            " [0.38543802 0.55499776 0.50469588 ... 0.43520547 0.51998738 0.36539528]\n",
            " ...\n",
            " [0.28819678 0.46406733 0.45136062 ... 0.52348112 0.51977772 0.31903569]\n",
            " [0.32042981 0.58142077 0.65604945 ... 0.57224097 0.51219919 0.27100913]\n",
            " [0.2795502  0.46720599 0.45870432 ... 0.3854164  0.31633473 0.13405514]]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "Ax = 50\n",
        "Ay = 50\n",
        "A = np.random.rand(Ax, Ay)\n",
        "B = np.zeros_like(A)\n",
        "\n",
        "#TODO: calculo del tiempo en cpu\n",
        "\n",
        "Mean_3x3_sequential(A, B)\n",
        "print(\"input\\n\", A)\n",
        "print(\"output\\n\", B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoXI_YDgBW6v",
        "outputId": "e292693e-8733-48d2-f035-f21599f64833"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input\n",
            " [[0.25343217 0.88453917 0.11655505 ... 0.58155069 0.3772749  0.65740254]\n",
            " [0.54746975 0.53939572 0.66098    ... 0.38421246 0.60397874 0.98743678]\n",
            " [0.49014738 0.66282884 0.66250477 ... 0.3242048  0.09154355 0.27138173]\n",
            " ...\n",
            " [0.04962472 0.31829174 0.66001656 ... 0.53019427 0.87677016 0.35581574]\n",
            " [0.82315862 0.5766718  0.82800638 ... 0.83812446 0.10684913 0.36040019]\n",
            " [0.43248398 0.68363741 0.8608957  ... 0.8023918  0.60685719 0.13238978]]\n",
            "output\n",
            " [[0.24720409 0.33359689 0.34379533 ... 0.35326126 0.39909511 0.2917881 ]\n",
            " [0.37531257 0.535317   0.51184538 ... 0.40565321 0.47544294 0.33211316]\n",
            " [0.38543802 0.55499776 0.50469589 ... 0.43520549 0.51998742 0.36539528]\n",
            " ...\n",
            " [0.2881968  0.46406735 0.4513606  ... 0.5234811  0.51977772 0.31903566]\n",
            " [0.3204298  0.58142074 0.65604941 ... 0.57224099 0.51219919 0.27100915]\n",
            " [0.27955018 0.46720595 0.45870431 ... 0.3854164  0.31633475 0.13405514]]\n",
            "memory\n",
            " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.25343218 0.88453919 ... 0.3772749  0.65740252 0.        ]\n",
            " [0.         0.54746974 0.53939569 ... 0.60397875 0.98743677 0.        ]\n",
            " ...\n",
            " [0.         0.82315862 0.57667178 ... 0.10684913 0.3604002  0.        ]\n",
            " [0.         0.43248397 0.68363744 ... 0.60685718 0.13238978 0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "B = np.zeros_like(A)\n",
        "B_aux = np.zeros_like(np.random.rand(Ax+2, Ay+2))\n",
        "\n",
        "A_device = cuda.to_device(A)\n",
        "B_device = cuda.to_device(B)\n",
        "aux_device = cuda.to_device(B_aux)\n",
        "\n",
        "threads_per_block = (16,16)\n",
        "\n",
        "blocks_X = math.ceil(B_aux.shape[0] / threads_per_block[0])\n",
        "blocks_Y = math.ceil(B_aux.shape[1] / threads_per_block[1])\n",
        "blocks_total = (blocks_X, blocks_Y)\n",
        "\n",
        "#TODO: calculo del tiempo en gpu\n",
        "\n",
        "Mean_3x3_parallel[blocks_total, threads_per_block](A_device, B_device, aux_device)\n",
        "\n",
        "B_par = B_device.copy_to_host()\n",
        "aux = aux_device.copy_to_host()\n",
        "\n",
        "print(\"input\\n\", A)\n",
        "print(\"output\\n\", B_par)\n",
        "\n",
        "#TODO: eliminación de la varaible auxiliar\n",
        "\n",
        "print(\"memory\\n\", aux)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO: calculo del speedup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNJsCS_POCqU"
      },
      "source": [
        "# Optional assignment #2: Reduction approaches (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yK9GZG6Wr7V",
        "outputId": "2d9f250e-6a25-4bfe-9231-50431c40b917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input\n",
            " [0.5488135  0.71518937 0.60276338 0.54488318 0.4236548  0.64589411\n",
            " 0.43758721 0.891773   0.96366276 0.38344152 0.79172504 0.52889492\n",
            " 0.56804456 0.92559664 0.07103606 0.0871293  0.0202184  0.83261985\n",
            " 0.77815675 0.87001215 0.97861834 0.79915856 0.46147936 0.78052918\n",
            " 0.11827443 0.63992102 0.14335329 0.94466892 0.52184832 0.41466194\n",
            " 0.26455561 0.77423369 0.45615033 0.56843395 0.0187898  0.6176355\n",
            " 0.61209572 0.616934   0.94374808 0.6818203 ]\n",
            "Output\n",
            " 22.988006826188062\n"
          ]
        }
      ],
      "source": [
        "def Reduce_sequential(input, output):\n",
        "    acum = 0\n",
        "    for i in range(input.shape[0]):\n",
        "        acum += input[i]\n",
        "    output[0] = acum\n",
        "\n",
        "\n",
        "N = 40\n",
        "np.random.seed(0)\n",
        "A = np.random.rand(N)\n",
        "output = np.zeros(1)\n",
        "\n",
        "#TODO: calculo del tiempo en cpu\n",
        "\n",
        "Reduce_sequential(A, output)\n",
        "\n",
        "print(\"Input\\n\", A)\n",
        "print(\"Output\\n\", output[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUrHrvK1Wk-d"
      },
      "source": [
        "## Reduction #1: Interleaved addressing with divergent branching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "twNQAnUJOCHb"
      },
      "outputs": [],
      "source": [
        "################################ Explicaciones pal iñigo ################################\n",
        "\n",
        "@cuda.jit\n",
        "def reduce_interleaved_divergent(input, output):\n",
        "    # Se piden crear dos listas en memoria compartida\n",
        "    sSrc = cuda.shared.array(16, dtype=float32)\n",
        "    sDst = cuda.shared.array(16, dtype=float32)\n",
        "\n",
        "    # Id del hilo en el bloque\n",
        "    tid = cuda.threadIdx.x\n",
        "    # Id del bloque del programa\n",
        "    bid = cuda.blockIdx.x\n",
        "    # Nº de hilos que tiene el bloque\n",
        "    bx = cuda.blockDim.x\n",
        "\n",
        "    # Copiamos los datos del input a la memoria compartida\n",
        "    if tid + bid * bx < input.size:\n",
        "        # Este es el metodo por excelencia para guardar los datos en una lista\n",
        "        # en el que no se solapa la posición con ningun otro hilo, en caso de\n",
        "        # no comprenderlo dimelo\n",
        "        sSrc[tid] = input[tid + bid * bx]\n",
        "    else:\n",
        "        # En caso de que el hilo no tenga posición del array no pone valor\n",
        "        sSrc[tid] = 0\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    # El stride es la división del hilo que se va multiplicando por 2 para que\n",
        "    # se vaya minimizando el uso de hilos, haciendo en cada iteración:\n",
        "    # nºhilos/2 -> nºhilos/4 ...\n",
        "    stride = 2\n",
        "    while stride <= bx:\n",
        "        if tid % stride == 0:\n",
        "            if tid + stride // 2 < bx:\n",
        "                # sSrc[tid + stride // 2] lo dan en el enunciado\n",
        "                sDst[tid] = sSrc[tid] + sSrc[tid + stride // 2]\n",
        "        cuda.syncthreads()\n",
        "\n",
        "        # Se copia la lista resultado a la lista de elementos para la siguiente\n",
        "        # iteración\n",
        "        sSrc[tid] = sDst[tid]\n",
        "        cuda.syncthreads()\n",
        "\n",
        "        # Como se indica arriba, se multiplica stride * 2\n",
        "        stride *= 2\n",
        "\n",
        "    # El primer hilo de todos tiene el privilegio de guardar el resultado\n",
        "    if tid == 0:\n",
        "        cuda.atomic.add(output, 0, sDst[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXk9iR_PQMnZ",
        "outputId": "8de8b5e2-ffed-4d17-989e-2f0b4e2ca69a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input\n",
            " [0.5488135  0.71518937 0.60276338 0.54488318 0.4236548  0.64589411\n",
            " 0.43758721 0.891773   0.96366276 0.38344152 0.79172504 0.52889492\n",
            " 0.56804456 0.92559664 0.07103606 0.0871293  0.0202184  0.83261985\n",
            " 0.77815675 0.87001215 0.97861834 0.79915856 0.46147936 0.78052918\n",
            " 0.11827443 0.63992102 0.14335329 0.94466892 0.52184832 0.41466194\n",
            " 0.26455561 0.77423369 0.45615033 0.56843395 0.0187898  0.6176355\n",
            " 0.61209572 0.616934   0.94374808 0.6818203 ]\n",
            "Output\n",
            " 22.988006591796875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 56 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ],
      "source": [
        "output = np.zeros(1)\n",
        "\n",
        "input_device = cuda.to_device(A)\n",
        "output_device = cuda.to_device(output)\n",
        "\n",
        "threads_per_block = 16\n",
        "blocks_per_grid = math.ceil(N + threads_per_block - 1 / threads_per_block)\n",
        "\n",
        "#TODO: calculo del tiempo en gpu\n",
        "\n",
        "reduce_interleaved_divergent[blocks_per_grid, threads_per_block](input_device, output_device)\n",
        "\n",
        "output = output_device.copy_to_host()\n",
        "\n",
        "print(\"Input\\n\", A)\n",
        "print(\"Output\\n\", output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO: calculo del speedup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfa_IwVUXVxp"
      },
      "source": [
        "## Reduction #2: Interleaved addressing with no divergent branching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L36WqhwRXHj0"
      },
      "outputs": [],
      "source": [
        "@cuda.jit\n",
        "def reduce_interleaved_no_divergent(input, output):\n",
        "    # Allocate shared memory based on the number of threads per block (bx)\n",
        "    sSrc = cuda.shared.array(shape=16, dtype=float32)  # Dynamically set the size in launch configuration\n",
        "    tid = cuda.threadIdx.x\n",
        "    bid = cuda.blockIdx.x\n",
        "    bx = cuda.blockDim.x\n",
        "\n",
        "    # Load data into shared memory with bounds check\n",
        "    idx = tid + bid * bx\n",
        "    if idx < input.size:\n",
        "        sSrc[tid] = input[idx]\n",
        "    else:\n",
        "        sSrc[tid] = 0.0  # Handle out-of-bounds threads\n",
        "    cuda.syncthreads()  # Ensure all threads have loaded their data\n",
        "\n",
        "    # Interleaved reduction without branching\n",
        "    stride = 1\n",
        "    while stride < bx:\n",
        "        index = tid + stride\n",
        "        if index < bx:  # Ensure we do not access out-of-bounds in shared memory\n",
        "            sSrc[tid] += sSrc[index]\n",
        "        cuda.syncthreads()  # Synchronize all threads before the next iteration\n",
        "        stride *= 2\n",
        "\n",
        "    # Only the first thread in each block writes the result to the output\n",
        "    if tid == 0:\n",
        "        cuda.atomic.add(output, 0, sSrc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS3a2SYlXLAY",
        "outputId": "d6f53ae1-a265-4202-ab06-1ee00448d8e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 56 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input\n",
            " [0.5488135  0.71518937 0.60276338 0.54488318 0.4236548  0.64589411\n",
            " 0.43758721 0.891773   0.96366276 0.38344152 0.79172504 0.52889492\n",
            " 0.56804456 0.92559664 0.07103606 0.0871293  0.0202184  0.83261985\n",
            " 0.77815675 0.87001215 0.97861834 0.79915856 0.46147936 0.78052918\n",
            " 0.11827443 0.63992102 0.14335329 0.94466892 0.52184832 0.41466194\n",
            " 0.26455561 0.77423369 0.45615033 0.56843395 0.0187898  0.6176355\n",
            " 0.61209572 0.616934   0.94374808 0.6818203 ]\n",
            "Output\n",
            " 22.988006591796875\n"
          ]
        }
      ],
      "source": [
        "output = np.zeros(1)\n",
        "\n",
        "input_device = cuda.to_device(A)\n",
        "output_device = cuda.to_device(output)\n",
        "\n",
        "threads_per_block = 16\n",
        "blocks_per_grid = math.ceil(N + threads_per_block - 1 / threads_per_block)\n",
        "\n",
        "#TODO: calculo del tiempo en gpu\n",
        "\n",
        "reduce_interleaved_no_divergent[blocks_per_grid, threads_per_block](input_device, output_device)\n",
        "\n",
        "output = output_device.copy_to_host()\n",
        "\n",
        "print(\"Input\\n\", A)\n",
        "print(\"Output\\n\", output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO: calculo del speedup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1v0BTGbX3kj"
      },
      "source": [
        "## Reduction #3: Sequential addressing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_fvyOQncX1I3"
      },
      "outputs": [],
      "source": [
        "@cuda.jit\n",
        "def reduce_sequential_addressing(input, output):\n",
        "    sSrc = cuda.shared.array(16, dtype=float32)\n",
        "\n",
        "    tid = cuda.threadIdx.x\n",
        "    bid = cuda.blockIdx.x\n",
        "    bx = cuda.blockDim.x\n",
        "\n",
        "    if tid + bid * bx < input.size:\n",
        "        sSrc[tid] = input[tid + bid * bx]\n",
        "    else:\n",
        "        sSrc[tid] = 0\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    stride = 1\n",
        "    while stride < bx:\n",
        "        index = 2 * stride * tid\n",
        "        if index + stride < bx:\n",
        "            sSrc[index] += sSrc[index + stride]\n",
        "        cuda.syncthreads()\n",
        "\n",
        "        stride *= 2\n",
        "\n",
        "    if tid == 0:\n",
        "        cuda.atomic.add(output, 0, sSrc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riO6z0hCX76H",
        "outputId": "0fe70824-b822-4fcd-97f2-3d2ee202ff60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 56 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input\n",
            " [0.5488135  0.71518937 0.60276338 0.54488318 0.4236548  0.64589411\n",
            " 0.43758721 0.891773   0.96366276 0.38344152 0.79172504 0.52889492\n",
            " 0.56804456 0.92559664 0.07103606 0.0871293  0.0202184  0.83261985\n",
            " 0.77815675 0.87001215 0.97861834 0.79915856 0.46147936 0.78052918\n",
            " 0.11827443 0.63992102 0.14335329 0.94466892 0.52184832 0.41466194\n",
            " 0.26455561 0.77423369 0.45615033 0.56843395 0.0187898  0.6176355\n",
            " 0.61209572 0.616934   0.94374808 0.6818203 ]\n",
            "Output\n",
            " 22.988006591796875\n"
          ]
        }
      ],
      "source": [
        "output = np.zeros(1)\n",
        "\n",
        "input_device = cuda.to_device(A)\n",
        "output_device = cuda.to_device(output)\n",
        "\n",
        "threads_per_block = 16\n",
        "blocks_per_grid = math.ceil(N + threads_per_block - 1 / threads_per_block)\n",
        "\n",
        "#TODO: calculo del tiempo en gpu\n",
        "\n",
        "reduce_sequential_addressing[blocks_per_grid, threads_per_block](input_device, output_device)\n",
        "\n",
        "output = output_device.copy_to_host()\n",
        "\n",
        "print(\"Input\\n\", A)\n",
        "print(\"Output\\n\", output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO: calculo del speedup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-wRqWP7YXAb"
      },
      "source": [
        "## Reduction #4: Atomic addition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3BOSmW_YQvQ"
      },
      "outputs": [],
      "source": [
        "@cuda.jit\n",
        "def reduce_atomic_addition(input, output):\n",
        "\n",
        "    partial_sum = cuda.shared.array(16, dtype=float32)\n",
        "\n",
        "    tid = cuda.threadIdx.x\n",
        "    bid = cuda.blockIdx.x\n",
        "    bx = cuda.blockDim.x\n",
        "\n",
        "    if tid == 0:\n",
        "        partial_sum[0] = 0.0\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    if tid + bid * bx < input.size:\n",
        "        cuda.atomic.add(partial_sum, 0, input[tid + bid * bx])\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    if tid == 0:\n",
        "        cuda.atomic.add(output, 0, partial_sum[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSUni8DwYea8",
        "outputId": "1a2c6d39-3bbb-4c0c-959e-3d0f71d6f868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input\n",
            " [0.5488135  0.71518937 0.60276338 0.54488318 0.4236548  0.64589411\n",
            " 0.43758721 0.891773   0.96366276 0.38344152 0.79172504 0.52889492\n",
            " 0.56804456 0.92559664 0.07103606 0.0871293  0.0202184  0.83261985\n",
            " 0.77815675 0.87001215 0.97861834 0.79915856 0.46147936 0.78052918\n",
            " 0.11827443 0.63992102 0.14335329 0.94466892 0.52184832 0.41466194\n",
            " 0.26455561 0.77423369 0.45615033 0.56843395 0.0187898  0.6176355\n",
            " 0.61209572 0.616934   0.94374808 0.6818203 ]\n",
            "Output\n",
            " 22.988006114959717\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 56 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ],
      "source": [
        "output = np.zeros(1)\n",
        "\n",
        "input_device = cuda.to_device(A)\n",
        "output_device = cuda.to_device(output)\n",
        "\n",
        "threads_per_block = 16\n",
        "blocks_per_grid = math.ceil(N + threads_per_block - 1 / threads_per_block)\n",
        "\n",
        "#TODO: calculo del tiempo en gpu\n",
        "\n",
        "reduce_atomic_addition[blocks_per_grid, threads_per_block](input_device, output_device)\n",
        "\n",
        "output = output_device.copy_to_host()\n",
        "\n",
        "print(\"Input\\n\", A)\n",
        "print(\"Output\\n\", output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO: calculo del speedup"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
